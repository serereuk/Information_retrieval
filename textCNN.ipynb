{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serereuk/Information_retrieval/blob/master/textCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6clxItSbIwdf",
        "colab_type": "text"
      },
      "source": [
        "# textCNN Rand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLH5fYvhZWWm",
        "colab_type": "code",
        "outputId": "f443ffea-ddb6-448c-860c-8b39c4cb98c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAlvzlYnJH6",
        "colab_type": "code",
        "outputId": "14251b57-880d-463e-d8ed-17bb83a7e294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MO3AHmqnKVE",
        "colab_type": "code",
        "outputId": "46f85ea0-3400-434d-9d12-641277199635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ4UyxP-o3sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, GlobalMaxPooling1D, ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOj4GYRIp6I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Reader(text_file_path):\n",
        "    data = [];\n",
        "    for i in text_file_path:\n",
        "        data += open(i, 'r', encoding=\"ISO-8859-1\").readlines()\n",
        "    label = [0] * int(len(data)/2) + [1] * int(len(data)/2)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = re.sub('^\\W+', \"\", data[i].lower().strip())\n",
        "    return data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnuWWKDBxDOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Indexing_padding(data, tok=None):\n",
        "    if tok == None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(data)\n",
        "    else:\n",
        "        tokenizer = tok\n",
        "    tensor = tokenizer.texts_to_sequences(data)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcfY8EIHv6tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, y = Reader(['rt-polarity.neg.txt', 'rt-polarity.pos.txt'])\n",
        "tensor, tokenizer = Indexing_padding(data, None)\n",
        "x_train, x_val, y_train, y_val = train_test_split(tensor, y, test_size=0.1, stratify=y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx-hU4BNcLuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Buffer_size = len(x_train) + 1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "Batch_size = 50\n",
        "epoch = 10\n",
        "learning_rate = 0.001\n",
        "dimension = 300\n",
        "tf.random.set_seed(1)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(Buffer_size)\n",
        "train_dataset = train_dataset.batch(Batch_size, drop_remainder=False)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(Buffer_size)\n",
        "test_dataset = test_dataset.batch(Batch_size, drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-nh2qe3JoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_46VNARZins",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textCNN(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, dimension=300):\n",
        "        super(textCNN, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension)\n",
        "        self.conv1 = Conv1D(100, 3, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv2 = Conv1D(100, 4, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv3 = Conv1D(100, 5, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.dense = Dense(2, activation='softmax', kernel_constraint=max_norm(3))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embed(x)\n",
        "        filter_result1 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv1(x))))\n",
        "        filter_result2 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv2(x))))\n",
        "        filter_result3 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv3(x))))\n",
        "        x = tf.concat([filter_result1, filter_result2, filter_result3], axis=1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "model = textCNN(vocab_size, dimension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0hb8NNy2Fez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, label):\n",
        "    with tf.GradientTape()as tape:\n",
        "        predictions = model(x)\n",
        "        loss = loss_object(label, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwQX4Efu7hUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(x, label):\n",
        "    predictions = model(x, training=False)\n",
        "    loss = loss_object(label, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfvAJ5bf_b3",
        "colab_type": "code",
        "outputId": "e0bc8714-492c-4693-d3c6-b36eb4bc353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for ep in range(epoch):\n",
        "    for x, label in train_dataset:\n",
        "        train_step(x,label)\n",
        "    for x, label in test_dataset:\n",
        "        test_step(x, label)\n",
        "    template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n",
        "    print (template.format(ep+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크: 1, 손실: 0.6140450239181519, 정확도: 67.8895263671875, 테스트 손실: 0.5566848516464233, 테스트 정확도: 73.66448211669922\n",
            "에포크: 2, 손실: 0.5235644578933716, 정확도: 78.15007781982422, 테스트 손실: 0.5576726794242859, 테스트 정확도: 73.52389526367188\n",
            "에포크: 3, 손실: 0.46739476919174194, 정확도: 84.16883850097656, 테스트 손실: 0.5549745559692383, 테스트 정확도: 74.03936767578125\n",
            "에포크: 4, 손실: 0.43333637714385986, 정확도: 87.74882507324219, 테스트 손실: 0.555284321308136, 테스트 정확도: 74.203369140625\n",
            "에포크: 5, 손실: 0.411521315574646, 정확도: 89.99896240234375, 테스트 손실: 0.555732786655426, 테스트 정확도: 74.24554443359375\n",
            "에포크: 6, 손실: 0.39659932255744934, 정확도: 91.52857971191406, 테스트 손실: 0.5551154613494873, 테스트 정확도: 74.24242401123047\n",
            "에포크: 7, 손실: 0.3857688903808594, 정확도: 92.63455963134766, 테스트 손실: 0.5548070073127747, 테스트 정확도: 74.30713653564453\n",
            "에포크: 8, 손실: 0.37750542163848877, 정확도: 93.47837829589844, 테스트 손실: 0.5554026961326599, 테스트 정확도: 74.34395599365234\n",
            "에포크: 9, 손실: 0.3710392713546753, 정확도: 94.13699340820312, 테스트 손실: 0.5560609698295593, 테스트 정확도: 74.23721313476562\n",
            "에포크: 10, 손실: 0.3658406436443329, 정확도: 94.6670150756836, 테스트 손실: 0.5564296841621399, 테스트 정확도: 74.18931579589844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJQZzCuWI0__",
        "colab_type": "text"
      },
      "source": [
        "# textCNN_static"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__ghhlO2zlTz",
        "colab_type": "code",
        "outputId": "99a408e9-2958-492a-cd41-0c53be509433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgTPjSFcKz7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "90099338-20c8-423b-bbe7-d9fa3413c9e2",
        "id": "2DaLbDfvx4_O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qk0tSqDvx4_c",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, GlobalMaxPooling1D, ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GUNyDDsZx4_q",
        "colab": {}
      },
      "source": [
        "def Reader(text_file_path):\n",
        "    data = [];\n",
        "    for i in text_file_path:\n",
        "        data += open(i, 'r', encoding=\"ISO-8859-1\").readlines()\n",
        "    label = [0] * int(len(data)/2) + [1] * int(len(data)/2)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = re.sub('^\\W+', \"\", data[i].lower().strip())\n",
        "    return data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eYBr11iTx4_u",
        "colab": {}
      },
      "source": [
        "def Indexing_padding(data, tok=None):\n",
        "    if tok == None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(data)\n",
        "    else:\n",
        "        tokenizer = tok\n",
        "    tensor = tokenizer.texts_to_sequences(data)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U-D5EwD3x4_5",
        "colab": {}
      },
      "source": [
        "data, y = Reader(['rt-polarity.neg.txt', 'rt-polarity.pos.txt'])\n",
        "tensor, tokenizer = Indexing_padding(data, None)\n",
        "x_train, x_val, y_train, y_val = train_test_split(tensor, y, test_size=0.1, stratify=y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B7WwRlGMx5AF",
        "colab": {}
      },
      "source": [
        "Buffer_size = len(x_train) + 1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "Batch_size = 50\n",
        "epoch = 10\n",
        "learning_rate = 0.001\n",
        "dimension = 300\n",
        "tf.random.set_seed(1)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(Buffer_size)\n",
        "train_dataset = train_dataset.batch(Batch_size, drop_remainder=False)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(Buffer_size)\n",
        "test_dataset = test_dataset.batch(Batch_size, drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BNiqYCTCx5AK",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_el8sWMfPV4",
        "colab_type": "code",
        "outputId": "d1c07e78-aee1-4cc2-dee9-e8b0d39ce7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/information/CNN, RNN/CNN/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agLkK6eUrqCI",
        "colab_type": "code",
        "outputId": "388c3e27-33ad-4251-bc2e-7b90105942ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "var = tf.Variable(tf.initializers.RandomUniform(-0.001, 0.001)(shape=(vocab_size, 300))).numpy()\n",
        "i = 0\n",
        "for index, word in zip(list(tokenizer.word_index.values()), list(tokenizer.word_index.keys())):\n",
        "    try:\n",
        "        var[index] = model.get_vector(word)\n",
        "        i += 1\n",
        "    except:\n",
        "        pass\n",
        "print(i, len(var))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15902 21374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MupRts7P5V2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNnGDKSbphYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textCNN_static(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, pretrained, dimension=300):\n",
        "        super(textCNN_static, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension, embeddings_initializer=tf.keras.initializers.Constant(pretrained),trainable=False)\n",
        "        self.conv1 = Conv1D(100, 3, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv2 = Conv1D(100, 4, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv3 = Conv1D(100, 5, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.dense = Dense(2, activation='softmax', kernel_constraint=max_norm(3))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embed(x)\n",
        "        filter_result1 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv1(x))))\n",
        "        filter_result2 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv2(x))))\n",
        "        filter_result3 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv3(x))))\n",
        "        x = tf.concat([filter_result1, filter_result2, filter_result3], axis=1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "model = textCNN_static(vocab_size,var, dimension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pKkU8CmhyMb1",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, label):\n",
        "    with tf.GradientTape()as tape:\n",
        "        predictions = model(x)\n",
        "        loss = loss_object(label, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ieTUsp9syMb5",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(x, label):\n",
        "    predictions = model(x, training=False)\n",
        "    loss = loss_object(label, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ddb9b83c-a600-41b0-d34c-943c8412b6ca",
        "id": "cXdFld2byMb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for ep in range(epoch):\n",
        "    for x, label in train_dataset:\n",
        "        train_step(x,label)\n",
        "    for x, label in test_dataset:\n",
        "        test_step(x, label)\n",
        "    template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n",
        "    print (template.format(ep+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크: 1, 손실: 0.5627406239509583, 정확도: 74.27827453613281, 테스트 손실: 0.5388481616973877, 테스트 정확도: 75.72633361816406\n",
            "에포크: 2, 손실: 0.5182666778564453, 정확도: 79.3434066772461, 테스트 손실: 0.5322895050048828, 테스트 정확도: 76.42924499511719\n",
            "에포크: 3, 손실: 0.48660263419151306, 정확도: 83.15094757080078, 테스트 손실: 0.5271875858306885, 테스트 정확도: 76.6323013305664\n",
            "에포크: 4, 손실: 0.46000170707702637, 정확도: 86.12037658691406, 테스트 손실: 0.52436363697052, 테스트 정확도: 77.10871887207031\n",
            "에포크: 5, 손실: 0.43882647156715393, 정확도: 88.33142852783203, 테스트 손실: 0.5227517485618591, 테스트 정확도: 77.33833312988281\n",
            "에포크: 6, 손실: 0.4223492741584778, 정확도: 89.96178436279297, 테스트 손실: 0.520948588848114, 테스트 정확도: 77.6788558959961\n",
            "에포크: 7, 손실: 0.4096299111843109, 정확도: 91.17546081542969, 테스트 손실: 0.5200791358947754, 테스트 정확도: 77.82836151123047\n",
            "에포크: 8, 손실: 0.39963528513908386, 정확도: 92.1143798828125, 테스트 손실: 0.5188255906105042, 테스트 정확도: 77.96392059326172\n",
            "에포크: 9, 손실: 0.39166054129600525, 정확도: 92.85623168945312, 테스트 손실: 0.518675684928894, 테스트 정확도: 78.04853057861328\n",
            "에포크: 10, 손실: 0.3851799964904785, 정확도: 93.45805358886719, 테스트 손실: 0.5179168581962585, 테스트 정확도: 78.19119262695312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3WT1G9uI-al",
        "colab_type": "text"
      },
      "source": [
        "# textCNN_non_static"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "99a408e9-2958-492a-cd41-0c53be509433",
        "id": "Yy4nr83fJFVu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ryt3uSsKylG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "90099338-20c8-423b-bbe7-d9fa3413c9e2",
        "id": "NKNe4q-kJFV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXoN5X-MJFV7",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, GlobalMaxPooling1D, ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bgUttQIGJFV9",
        "colab": {}
      },
      "source": [
        "def Reader(text_file_path):\n",
        "    data = [];\n",
        "    for i in text_file_path:\n",
        "        data += open(i, 'r', encoding=\"ISO-8859-1\").readlines()\n",
        "    label = [0] * int(len(data)/2) + [1] * int(len(data)/2)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = re.sub('^\\W+', \"\", data[i].lower().strip())\n",
        "    return data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iu-EGsBXJFV_",
        "colab": {}
      },
      "source": [
        "def Indexing_padding(data, tok=None):\n",
        "    if tok == None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(data)\n",
        "    else:\n",
        "        tokenizer = tok\n",
        "    tensor = tokenizer.texts_to_sequences(data)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cpmDEyp4JFWB",
        "colab": {}
      },
      "source": [
        "data, y = Reader(['rt-polarity.neg.txt', 'rt-polarity.pos.txt'])\n",
        "tensor, tokenizer = Indexing_padding(data, None)\n",
        "x_train, x_val, y_train, y_val = train_test_split(tensor, y, test_size=0.1, stratify=y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OQiWkjCJFWD",
        "colab": {}
      },
      "source": [
        "Buffer_size = len(x_train) + 1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "Batch_size = 50\n",
        "epoch = 10\n",
        "learning_rate = 0.001\n",
        "dimension = 300\n",
        "tf.random.set_seed(1)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(Buffer_size)\n",
        "train_dataset = train_dataset.batch(Batch_size, drop_remainder=False)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(Buffer_size)\n",
        "test_dataset = test_dataset.batch(Batch_size, drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tR_UMbB6JFWG",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d1c07e78-aee1-4cc2-dee9-e8b0d39ce7ed",
        "id": "J-vD-nCyJFWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/information/CNN, RNN/CNN/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "388c3e27-33ad-4251-bc2e-7b90105942ba",
        "id": "YkMlDX7QJFWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "var = tf.Variable(tf.initializers.RandomUniform(-0.001, 0.001)(shape=(vocab_size, 300))).numpy()\n",
        "i = 0\n",
        "for index, word in zip(list(tokenizer.word_index.values()), list(tokenizer.word_index.keys())):\n",
        "    try:\n",
        "        var[index] = model.get_vector(word)\n",
        "        i += 1\n",
        "    except:\n",
        "        pass\n",
        "print(i, len(var))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15902 21374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kN1kRX3cJFWO",
        "colab": {}
      },
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmPuzYQnyjqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textCNN_non_static(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, pretrained, dimension=300):\n",
        "        super(textCNN_non_static, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension, embeddings_initializer=tf.keras.initializers.Constant(pretrained),trainable=True)\n",
        "        self.conv1 = Conv1D(100, 3, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv2 = Conv1D(100, 4, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv3 = Conv1D(100, 5, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.dense = Dense(2, activation='softmax', kernel_constraint=max_norm(3))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embed(x)\n",
        "        filter_result1 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv1(x))))\n",
        "        filter_result2 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv2(x))))\n",
        "        filter_result3 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv3(x))))\n",
        "        x = tf.concat([filter_result1, filter_result2, filter_result3], axis=1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "model = textCNN_non_static(vocab_size,var, dimension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ch6x_59u48z9",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, label):\n",
        "    with tf.GradientTape()as tape:\n",
        "        predictions = model(x)\n",
        "        loss = loss_object(label, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KzdK1u5K480C",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(x, label):\n",
        "    predictions = model(x, training=False)\n",
        "    loss = loss_object(label, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r67UXkV4503",
        "colab_type": "code",
        "outputId": "18e7d4ac-2bf9-471e-b87a-5f153aeedfc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for ep in range(epoch):\n",
        "    for x, label in train_dataset:\n",
        "        train_step(x,label)\n",
        "    for x, label in test_dataset:\n",
        "        test_step(x, label)\n",
        "    template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n",
        "    print (template.format(ep+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크: 1, 손실: 0.5502836108207703, 정확도: 75.14330291748047, 테스트 손실: 0.5157164335250854, 테스트 정확도: 78.912841796875\n",
            "에포크: 2, 손실: 0.48189282417297363, 정확도: 82.87649536132812, 테스트 손실: 0.5163614153862, 테스트 정확도: 78.53795623779297\n",
            "에포크: 3, 손실: 0.43908894062042236, 정확도: 87.52822875976562, 테스트 손실: 0.5117866396903992, 테스트 정확도: 78.912841796875\n",
            "에포크: 4, 손실: 0.4124760627746582, 정확도: 90.24231719970703, 테스트 손실: 0.5113073587417603, 테스트 정확도: 78.93627166748047\n",
            "에포크: 5, 손실: 0.3949950039386749, 정확도: 91.98957824707031, 테스트 손실: 0.5102297067642212, 테스트 정확도: 79.08153533935547\n",
            "에포크: 6, 손실: 0.38281187415122986, 정확도: 93.19437408447266, 테스트 손실: 0.50956791639328, 테스트 정확도: 78.92845916748047\n",
            "에포크: 7, 손실: 0.3738698959350586, 정확도: 94.07429504394531, 테스트 손실: 0.5091556310653687, 테스트 정확도: 78.92623138427734\n",
            "에포크: 8, 손실: 0.3670634925365448, 정확도: 94.74205017089844, 테스트 손실: 0.5089240670204163, 테스트 정확도: 78.901123046875\n",
            "에포크: 9, 손실: 0.3617025315761566, 정확도: 95.2672119140625, 테스트 손실: 0.5088534951210022, 테스트 정확도: 78.92325592041016\n",
            "에포크: 10, 손실: 0.3573867082595825, 정확도: 95.68942260742188, 테스트 손실: 0.5091304779052734, 테스트 정확도: 78.87535095214844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRDrOBBuJMIS",
        "colab_type": "text"
      },
      "source": [
        "# textCNN_multichannel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f5d57a59-78eb-4488-87b9-0d4f5496a065",
        "id": "yStIYfQwJPnX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POFWMIjhK4FQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "31d86e5f-d6ca-4c45-8a4b-63553aad2112"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8aeb4f6e-48c3-4c74-c40b-8191b4f8be0a",
        "id": "CBGGqEkaJPna",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dZgZZsV0JPnc",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.layers import Embedding, Conv2D, Dense, Dropout, GlobalMaxPooling2D, ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXSo8W9AJPnd",
        "colab": {}
      },
      "source": [
        "def Reader(text_file_path):\n",
        "    data = [];\n",
        "    for i in text_file_path:\n",
        "        data += open(i, 'r', encoding=\"ISO-8859-1\").readlines()\n",
        "    label = [0] * int(len(data)/2) + [1] * int(len(data)/2)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = re.sub('^\\W+', \"\", data[i].lower().strip())\n",
        "    return data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "svHvfGT8JPnh",
        "colab": {}
      },
      "source": [
        "def Indexing_padding(data, tok=None):\n",
        "    if tok == None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(data)\n",
        "    else:\n",
        "        tokenizer = tok\n",
        "    tensor = tokenizer.texts_to_sequences(data)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lBwaCgNJPnk",
        "colab": {}
      },
      "source": [
        "data, y = Reader(['rt-polarity.neg.txt', 'rt-polarity.pos.txt'])\n",
        "tensor, tokenizer = Indexing_padding(data, None)\n",
        "x_train, x_val, y_train, y_val = train_test_split(tensor, y, test_size=0.1, stratify=y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DYP9A2hoJPnp",
        "colab": {}
      },
      "source": [
        "Buffer_size = len(x_train) + 1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "Batch_size = 50\n",
        "epoch = 10\n",
        "learning_rate = 0.001\n",
        "dimension = 300\n",
        "tf.random.set_seed(1)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(Buffer_size)\n",
        "train_dataset = train_dataset.batch(Batch_size, drop_remainder=False)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(Buffer_size)\n",
        "test_dataset = test_dataset.batch(Batch_size, drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FN2Y-X6LJPnq",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a70a15d1-6f85-4ca0-846b-1c3503275251",
        "id": "dHH33vCrJPnu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/information/CNN, RNN/CNN/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea39c398-c449-432c-9210-979a41b7296c",
        "id": "0aBo_6EQJPnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "var = tf.Variable(tf.initializers.RandomUniform(-0.001, 0.001)(shape=(vocab_size, 300))).numpy()\n",
        "i = 0\n",
        "for index, word in zip(list(tokenizer.word_index.values()), list(tokenizer.word_index.keys())):\n",
        "    try:\n",
        "        var[index] = model.get_vector(word)\n",
        "        i += 1\n",
        "    except:\n",
        "        pass\n",
        "print(i, len(var))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15902 21374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O7LnQ-QSJPn2",
        "colab": {}
      },
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLLTmQoe5FUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textCNN_multi(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, pretrained, dimension=300):\n",
        "        super(textCNN_multi, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension, embeddings_initializer=tf.keras.initializers.Constant(pretrained),trainable=True)\n",
        "        self.embed2 = Embedding(vocab_size, dimension, embeddings_initializer=tf.keras.initializers.Constant(pretrained),trainable=False)\n",
        "        self.conv1 = Conv2D(100, (3, dimension), strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1,2]))\n",
        "        self.conv2 = Conv2D(100, (4, dimension), strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1,2]))\n",
        "        self.conv3 = Conv2D(100, (5, dimension), strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1,2]))\n",
        "        self.dense = Dense(2, activation='softmax', kernel_constraint=max_norm(3))\n",
        "\n",
        "    def call(self, x):\n",
        "        x1 = self.embed(x)\n",
        "        x2 = self.embed2(x)\n",
        "        x = tf.stack([x1, x2], axis=3)\n",
        "        filter_result1 = Dropout(0.5)(GlobalMaxPooling2D()(ReLU()(self.conv1(x))))\n",
        "        filter_result2 = Dropout(0.5)(GlobalMaxPooling2D()(ReLU()(self.conv2(x))))\n",
        "        filter_result3 = Dropout(0.5)(GlobalMaxPooling2D()(ReLU()(self.conv3(x))))\n",
        "        x = tf.concat([filter_result1, filter_result2, filter_result3], axis=1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "model = textCNN_multi(vocab_size, var, dimension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2LYteH7aK6l0",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, label):\n",
        "    with tf.GradientTape()as tape:\n",
        "        predictions = model(x)\n",
        "        loss = loss_object(label, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGZvuVszK6l3",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(x, label):\n",
        "    predictions = model(x, training=False)\n",
        "    loss = loss_object(label, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b950eb05-e4d2-467c-dd99-57c1f663511a",
        "id": "oDJGRbtMK6l8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for ep in range(epoch):\n",
        "    for x, label in train_dataset:\n",
        "        train_step(x,label)\n",
        "    for x, label in test_dataset:\n",
        "        test_step(x, label)\n",
        "    template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n",
        "    print (template.format(ep+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크: 1, 손실: 0.5381555557250977, 정확도: 76.0500259399414, 테스트 손실: 0.5221347212791443, 테스트 정확도: 76.38237762451172\n",
            "에포크: 2, 손실: 0.48139986395835876, 정확도: 82.42314147949219, 테스트 손실: 0.514563798904419, 테스트 정확도: 77.6476058959961\n",
            "에포크: 3, 손실: 0.441375732421875, 정확도: 86.77088928222656, 테스트 손실: 0.5135930776596069, 테스트 정확도: 77.81942749023438\n",
            "에포크: 4, 손실: 0.4157598912715912, 정확도: 89.48670959472656, 테스트 손실: 0.5127797722816467, 테스트 정확도: 78.1396484375\n",
            "에포크: 5, 손실: 0.3986738622188568, 정확도: 91.27045440673828, 테스트 손실: 0.512876570224762, 테스트 정확도: 78.29428100585938\n",
            "에포크: 6, 손실: 0.38665318489074707, 정확도: 92.51693725585938, 테스트 손실: 0.5133378505706787, 테스트 정확도: 78.27241516113281\n",
            "에포크: 7, 손실: 0.3777911365032196, 정확도: 93.42961120605469, 테스트 손실: 0.5142425894737244, 테스트 정확도: 78.27018737792969\n",
            "에포크: 8, 손실: 0.370988130569458, 정확도: 94.12715148925781, 테스트 손실: 0.5148140788078308, 테스트 정확도: 78.23336791992188\n",
            "에포크: 9, 손실: 0.3655976951122284, 정확도: 94.67894744873047, 테스트 손실: 0.515252947807312, 테스트 정확도: 78.24637603759766\n",
            "에포크: 10, 손실: 0.36124980449676514, 정확도: 95.12349700927734, 테스트 손실: 0.5155510902404785, 테스트 정확도: 78.22867584228516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJa_kQG2LxP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}