{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serereuk/Information_retrieval/blob/master/textCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLH5fYvhZWWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAlvzlYnJH6",
        "colab_type": "code",
        "outputId": "4673f4af-0bda-4144-bc73-9d60398752da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MO3AHmqnKVE",
        "colab_type": "code",
        "outputId": "a7398a71-c708-458d-f2a0-eaa26301579a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata'"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/information/CNN, RNN/CNN/rt-polaritydata(MR)/rt-polaritydata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ4UyxP-o3sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, GlobalMaxPooling1D, ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOj4GYRIp6I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Reader(text_file_path):\n",
        "    data = [];\n",
        "    for i in text_file_path:\n",
        "        data += open(i, 'r', encoding=\"ISO-8859-1\").readlines()\n",
        "    label = [0] * int(len(data)/2) + [1] * int(len(data)/2)\n",
        "    for i in range(len(data)):\n",
        "        data[i] = re.sub('^\\W+', \"\", data[i].lower().strip())\n",
        "    return data, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnuWWKDBxDOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Indexing_padding(data, tok=None):\n",
        "    if tok == None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(data)\n",
        "    else:\n",
        "        tokenizer = tok\n",
        "    tensor = tokenizer.texts_to_sequences(data)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcfY8EIHv6tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, y = Reader(['rt-polarity.neg.txt', 'rt-polarity.pos.txt'])\n",
        "tensor, tokenizer = Indexing_padding(data, None)\n",
        "x_train, x_val, y_train, y_val = train_test_split(tensor, y, test_size=0.1, stratify=y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx-hU4BNcLuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Buffer_size = len(x_train) + 1\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "Batch_size = 50\n",
        "epoch = 10\n",
        "learning_rate = 0.001\n",
        "dimension = 300\n",
        "tf.random.set_seed(1)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(Buffer_size)\n",
        "train_dataset = train_dataset.batch(Batch_size, drop_remainder=False)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(Buffer_size)\n",
        "test_dataset = test_dataset.batch(Batch_size, drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-nh2qe3JoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_46VNARZins",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class textCNN(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, dimension=300):\n",
        "        super(textCNN, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension)\n",
        "        self.conv1 = Conv1D(100, 3, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv2 = Conv1D(100, 4, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.conv3 = Conv1D(100, 5, strides=1, padding='valid', kernel_constraint=max_norm(3, axis=[0,1]))\n",
        "        self.dense = Dense(2, activation='softmax', kernel_constraint=max_norm(3))\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embed(x)\n",
        "        filter_result1 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv1(x))))\n",
        "        filter_result2 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv2(x))))\n",
        "        filter_result3 = Dropout(0.5)(GlobalMaxPooling1D()(ReLU()(self.conv3(x))))\n",
        "        x = tf.concat([filter_result1, filter_result2, filter_result3], axis=1)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "model = textCNN(vocab_size, dimension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0hb8NNy2Fez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, label):\n",
        "    with tf.GradientTape()as tape:\n",
        "        predictions = model(x)\n",
        "        loss = loss_object(label, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwQX4Efu7hUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(x, label):\n",
        "    predictions = model(x, training=False)\n",
        "    loss = loss_object(label, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(label, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfvAJ5bf_b3",
        "colab_type": "code",
        "outputId": "e0bc8714-492c-4693-d3c6-b36eb4bc353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for ep in range(epoch):\n",
        "    for x, label in train_dataset:\n",
        "        train_step(x,label)\n",
        "    for x, label in test_dataset:\n",
        "        test_step(x, label)\n",
        "    template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n",
        "    print (template.format(ep+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크: 1, 손실: 0.6140450239181519, 정확도: 67.8895263671875, 테스트 손실: 0.5566848516464233, 테스트 정확도: 73.66448211669922\n",
            "에포크: 2, 손실: 0.5235644578933716, 정확도: 78.15007781982422, 테스트 손실: 0.5576726794242859, 테스트 정확도: 73.52389526367188\n",
            "에포크: 3, 손실: 0.46739476919174194, 정확도: 84.16883850097656, 테스트 손실: 0.5549745559692383, 테스트 정확도: 74.03936767578125\n",
            "에포크: 4, 손실: 0.43333637714385986, 정확도: 87.74882507324219, 테스트 손실: 0.555284321308136, 테스트 정확도: 74.203369140625\n",
            "에포크: 5, 손실: 0.411521315574646, 정확도: 89.99896240234375, 테스트 손실: 0.555732786655426, 테스트 정확도: 74.24554443359375\n",
            "에포크: 6, 손실: 0.39659932255744934, 정확도: 91.52857971191406, 테스트 손실: 0.5551154613494873, 테스트 정확도: 74.24242401123047\n",
            "에포크: 7, 손실: 0.3857688903808594, 정확도: 92.63455963134766, 테스트 손실: 0.5548070073127747, 테스트 정확도: 74.30713653564453\n",
            "에포크: 8, 손실: 0.37750542163848877, 정확도: 93.47837829589844, 테스트 손실: 0.5554026961326599, 테스트 정확도: 74.34395599365234\n",
            "에포크: 9, 손실: 0.3710392713546753, 정확도: 94.13699340820312, 테스트 손실: 0.5560609698295593, 테스트 정확도: 74.23721313476562\n",
            "에포크: 10, 손실: 0.3658406436443329, 정확도: 94.6670150756836, 테스트 손실: 0.5564296841621399, 테스트 정확도: 74.18931579589844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf3JnlPBNfyq",
        "colab_type": "code",
        "outputId": "fdb32271-4c23-40aa-b9a2-83b524fc1a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"text_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  6412200   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              multiple                  90100     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            multiple                  120100    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            multiple                  150100    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  602       \n",
            "=================================================================\n",
            "Total params: 6,773,102\n",
            "Trainable params: 6,773,102\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_el8sWMfPV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}