{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serereuk/Information_retrieval/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuhA686Vlee6",
        "colab_type": "code",
        "outputId": "1e757ce1-da6b-4f9e-e227-4201983ba070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxS4taL1mU0M",
        "colab_type": "code",
        "outputId": "00cd286e-db8a-4f6f-d776-1927b6d19ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmBJryFmiVU",
        "colab_type": "code",
        "outputId": "900d0d93-ae6e-4705-ab8e-d513e2a16a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/information/CNN, RNN/RNN dataset'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/information/CNN, RNN/RNN dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrGp5UNRmViA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E8PIRL8nKZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Reader(file_path):\n",
        "    data = open(file_path, 'r').readlines()\n",
        "    en = []; fr = []\n",
        "    for i in data:\n",
        "        temp = i.strip().split('\\t')\n",
        "        en.append('<시작> ' + temp[0] + ' <끝>')\n",
        "        fr.append('<시작> ' + temp[1] + ' <끝>')\n",
        "    return en, fr\n",
        "\n",
        "def tokenizer(data, max_len=30):\n",
        "    tok = Tokenizer(filters='')\n",
        "    tok.fit_on_texts(data)\n",
        "    tensor = tok.texts_to_sequences(data)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=max_len, padding='post')\n",
        "    return tensor, tok\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDxq0WCao7jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_en, train_fr = Reader('eng-fra_train.txt')\n",
        "test_en, test_fr = Reader('eng-fra_test.txt')\n",
        "train_en_tokened, en_tok = tokenizer(train_en)\n",
        "train_fr_tokened, fr_tok = tokenizer(train_fr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04a_zS33rQT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Buffer_size = len(train_en_tokened) + 1\n",
        "Epoch = 50\n",
        "Batch_size = 512\n",
        "embed_dimension = 300\n",
        "steps_per_epoch = len(train_en_tokened)//Batch_size\n",
        "hidden_dimension = 512\n",
        "learning_rate = 0.005\n",
        "vocab_size = len(en_tok.word_index) + 1\n",
        "tf.random.set_seed(1)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_en_tokened, train_fr_tokened)).shuffle(Buffer_size)\n",
        "train_dataset = train_dataset.batch(Batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apL5qBV3t9eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, dimension, hidden_dimension):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension)\n",
        "        self.encode = LSTM(hidden_dimension, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x, state):\n",
        "        x = self.embed(x)\n",
        "        output, hidden_state, cell_state = self.encode(x, initial_state=state)\n",
        "        return output, [hidden_state, cell_state]\n",
        "        \n",
        "encoder = Encoder(vocab_size, 300, 512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eudMs12YvDAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(Model):\n",
        "\n",
        "    def __init__(self, vocab_size, dimension, hidden_dimension):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = Embedding(vocab_size, dimension)\n",
        "        self.decode = LSTM(hidden_dimension, return_state=True)\n",
        "        self.fc = Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, state, encoder_output):\n",
        "        x = self.embed(x)\n",
        "        [_, hidden_state, cell_state] = self.decode(x, initial_state = state)\n",
        "        hidden_state_cal = tf.expand_dims(hidden_state, 1)\n",
        "        attention = tf.nn.softmax(tf.reduce_sum(encoder_output*hidden_state_cal, 2), axis=1)\n",
        "        attention = tf.reduce_sum(encoder_output * tf.expand_dims(attention, 2), 2)\n",
        "        x = self.fc(tf.concat([attention, hidden_state], axis=1))\n",
        "        return x, [hidden_state, cell_state]\n",
        "\n",
        "decoder = Decoder(len(fr_tok.word_index)+1, 300, 512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EirxZQDQuny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_state):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_state = encoder(inp, enc_state)\n",
        "\n",
        "    dec_state = enc_state\n",
        "\n",
        "    dec_input = tf.expand_dims([fr_tok.word_index['<시작>']] * Batch_size, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_state  = decoder(dec_input, dec_state, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEE8extv0D3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_result(inp):\n",
        "    enc_state = [tf.zeros((1, 512)), tf.zeros((1,512))]\n",
        "    enc_output, enc_state = encoder(inp, enc_state)\n",
        "    dec_state = enc_state\n",
        "    dec_input = tf.expand_dims([fr_tok.word_index['<시작>']], 0)\n",
        "    ins = ''\n",
        "    for t in range(30):\n",
        "        if inp.numpy()[0][t] == 0:\n",
        "            break\n",
        "        ins += en_tok.index_word[inp.numpy()[0][t]] + ' '\n",
        "    print(ins)\n",
        "    result = ''\n",
        "    for _ in range(30):\n",
        "        prediction, dec_state = decoder(dec_input, dec_state, enc_output)\n",
        "        predicted_id = tf.argmax(prediction[0]).numpy()\n",
        "        a = fr_tok.index_word[predicted_id]\n",
        "        result += a + ' '\n",
        "        if a == '<끝>':\n",
        "            break\n",
        "        dec_input = tf.expand_dims([fr_tok.word_index[a]], 0)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLAFc6xkx-O8",
        "colab_type": "code",
        "outputId": "f180da3f-ac0e-4045-83c9-c4c3423780cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  enc_state = [tf.zeros((Batch_size, 512)), tf.zeros((Batch_size, 512))]\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_state)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  \n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.9323\n",
            "Epoch 1 Batch 100 Loss 0.5687\n",
            "Epoch 1 Loss 0.8455\n",
            "Epoch 2 Batch 0 Loss 0.5552\n",
            "Epoch 2 Batch 100 Loss 0.4596\n",
            "Epoch 2 Loss 0.5021\n",
            "Epoch 3 Batch 0 Loss 0.4190\n",
            "Epoch 3 Batch 100 Loss 0.3790\n",
            "Epoch 3 Loss 0.4072\n",
            "Epoch 4 Batch 0 Loss 0.3591\n",
            "Epoch 4 Batch 100 Loss 0.3247\n",
            "Epoch 4 Loss 0.3486\n",
            "Epoch 5 Batch 0 Loss 0.2865\n",
            "Epoch 5 Batch 100 Loss 0.3292\n",
            "Epoch 5 Loss 0.3078\n",
            "Epoch 6 Batch 0 Loss 0.2888\n",
            "Epoch 6 Batch 100 Loss 0.2934\n",
            "Epoch 6 Loss 0.2778\n",
            "Epoch 7 Batch 0 Loss 0.2339\n",
            "Epoch 7 Batch 100 Loss 0.2530\n",
            "Epoch 7 Loss 0.2555\n",
            "Epoch 8 Batch 0 Loss 0.2181\n",
            "Epoch 8 Batch 100 Loss 0.2538\n",
            "Epoch 8 Loss 0.2373\n",
            "Epoch 9 Batch 0 Loss 0.2044\n",
            "Epoch 9 Batch 100 Loss 0.2221\n",
            "Epoch 9 Loss 0.2202\n",
            "Epoch 10 Batch 0 Loss 0.1981\n",
            "Epoch 10 Batch 100 Loss 0.2218\n",
            "Epoch 10 Loss 0.2088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xjlu7Hn10Jr",
        "colab_type": "code",
        "outputId": "efdf0f59-9750-4f26-a6ea-11aadd4922f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_result(tf.expand_dims(inp[5], 0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<시작> tom didn t help me . <끝> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tom n y peut rien . <끝> '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR7fv-QjJXAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}